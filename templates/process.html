<div id="process" class="tall banner bgbanner">

        <img src="/static/images/bagpipes.jpg" class="banner-img">
        <div class="vmiddle">Modeling Approaches</div>
    
    </div>
<div class="tall clear">
</div>

<div >
    <p>
        Having reduced the playlist prediction problem to a classification problem, there were a number of models we could try to use. However, although we now have a classification problem we felt a decision tree would not work well for our problem. For each playlist, our model will use the mean of the playlist's songs' predictors to classify songs as belonging in a playlist or not belonging in the playlist. However, these mean quantities vary from playlist  to playlist so a decision tree which uses set thresholds to classify inputs would not be appropriate.
    </p>
    <p>
        Instead, the two approaches we decided on were first to build a simple logistic regression model, and then to see if we could build a neural network with higher accuracy
    </p>
    <h3>Logistic Regression</h3>
    <p>
        We ran fit a logistic regression model over 250 iterations on our training data. This model was mostly a test to see if our dataset had the predictors to allow us to achieve any significant level of accuracy in classification.
    </p>
<pre class="prettyprint code">
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=10000, solver='newton-cg', max_iter=250)
logreg.fit(train_data_x, train_data_y)
logreg.score(test_data_x, test_data_y)</pre>
    <p>
        Initially, we fit our model on training data where 1/3 of the playlist's original songs were included with response value 1, and 30 randomly selected songs from a set of songs we had gathered were used with response value 0. This model achieved 75% test accuracy with no cross-validation.
    </p>
    <p>
        However, while we were happy with the above model, it was fundamentally flawed for prediction. The model had fairly high accuracy of detecting if a song was originally in the playlist, but this assumed that the songs we'd "want" in a playlist would be in our set of testing songs for prediction. The odds of this occuring would be very low since we were randomly selecting songs from an arbitrary set of songs. Instead, as described in 5.1, we first queried the Spotify playlist for 50 songs from the playlist's most common genre and year (+/- 1 year).
    </p>
    <p>
        This brought our test accuracy down to 47.6%. The decrease was not surprising since by querying songs from the same genre as our playlist, we'll likely see smaller variations in features between the songs originally in the playlist and new songs. However, we decided this was a trade-off we were willing to make since the overall goal of the project is playlist prediction, and so its important that the pool of songs from which we are selecting songs to add to our playlist has a high probability of having a song we want, rather than letting there be a probability no song we want is in the pool. We will discuss results more in the next section.
    </p>
    <p>We attempted to implement cross-validation to improve our logistic regression model but ran into time-out errors that
        we were unable to fix before submission
    </p>
<pre class="prettyprint code">
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import cross_val_score
logregcv = LogisticRegressionCV(solver='saga', cv=5, max_iter=1000).fit(train_data_x, train_data_y)
</pre>
    <h3>Neural Network</h3>
    <p>Our second approach was to implement a Neural Network. We used the base code that we used in class and attempted
        modify it to improve our accuracy. We use binary_accuracy to measure our accuracy since we had reduced the problem
        to a binary classification problem.
    </p>
<pre class="prettyprint code">
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import GaussianNoise
from keras import regularizers
from sklearn.metrics import r2_score as r2

input_dim = len(columns)
n_hidden = 100

model = Sequential()
model.add(Dense(n_hidden, activation='relu', input_shape=(input_dim,)))
model.add(Dense(n_hidden, activation='sigmoid'))
model.add(Dense(n_hidden, activation='relu'))
model.add(Dense(n_hidden, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(optimizer='sgd', loss='binary_crossentropy',  metrics=['binary_accuracy'])
model.summary()
</pre>
<p>We added additional layers and tried changing the number of nodes at each hidden layer, mostly through trial and error.
    We were not able to improve the model much. We did discovere that linear activation made our model perform significantly
    worse. Results are discussed below
</p>
</div>
